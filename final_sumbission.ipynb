{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a1702cc",
   "metadata": {},
   "source": [
    "# install necessary libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "feb2ff8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: cvzone in c:\\users\\raj\\anaconda3\\lib\\site-packages (1.5.6)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\raj\\anaconda3\\lib\\site-packages (2.12.0)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\raj\\anaconda3\\lib\\site-packages (4.7.0.72)\n",
      "Requirement already satisfied: numpy in c:\\users\\raj\\anaconda3\\lib\\site-packages (from cvzone) (1.23.5)\n",
      "Requirement already satisfied: tensorflow-intel==2.12.0 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from tensorflow) (2.12.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.51.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\raj\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (23.0)\n",
      "Requirement already satisfied: tensorboard<2.13,>=2.12 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.12.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.4.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (3.8.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\raj\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (4.5.0)\n",
      "Requirement already satisfied: jax>=0.3.15 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\raj\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (65.6.3)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (4.22.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: keras<2.13,>=2.12.0 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.12.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (23.3.3)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (16.0.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.12.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.12.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: scipy>=1.5 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow) (1.10.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.16.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.7.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.4.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (5.3.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (1.26.14)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\raj\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install cvzone tensorflow opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4101b254",
   "metadata": {},
   "source": [
    "# Script to create a dataset and save in folder for each alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8a3f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from cvzone.HandTrackingModule import HandDetector\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "\n",
    "offset = 100  #Determine offset\n",
    "imgSize = 300 #Determone image size\n",
    "counter = 0\n",
    "\n",
    "#Change folders from A-z\n",
    "folder = \"C:/Users/Raj/Desktop/CVIP project/ASl_data/A\"  \n",
    "cap = cv2.VideoCapture(0)        #Start webcam\n",
    "detector = HandDetector(maxHands = 1)  #Set the maximum number of hands to 1\n",
    "while True:\n",
    "    success, img = cap.read()                      #read the images frim the webcam\n",
    "    hands, img = detector.findHands(img)           #Detect hands\n",
    "    cv2.imshow(\"imm\",img)\n",
    "    if hands:\n",
    "        hand1 = hands[0]                             \n",
    "        x,y,w,h = hand1['bbox']\n",
    "        imgWhite = np.ones((imgSize,imgSize,3),np.uint8)*255\n",
    "        #Crop the image to only save the data within region of interest i.e hand\n",
    "        imgCrop = img[max(0,y-offset):y+h+offset,max(0,x-offset):x+w+offset]     \n",
    "        aspectRatio = h/w\n",
    "        #Resize the image to fit to size of 300*300 while still keeping all the features of hand\n",
    "        if aspectRatio > 1:                                     \n",
    "            k = imgSize/h\n",
    "            wCal = math.ceil(k*w)\n",
    "            imgResize =cv2.resize(imgCrop,(wCal,imgSize))                       \n",
    "            imgResizeShape = imgResize.shape\n",
    "            wGap = math.ceil((imgSize - wCal)/2)\n",
    "            imgWhite[:,wGap:wCal+wGap] = imgResize\n",
    "        else:\n",
    "            k = imgSize/w\n",
    "            hCal = math.ceil(k*h)\n",
    "            imgResize =cv2.resize(imgCrop,(imgSize,hCal))                        \n",
    "            hGap = math.ceil((imgSize - hCal)/2)\n",
    "            b = hGap + hCal\n",
    "            imgWhite[hGap:hCal+hGap,:] = imgResize\n",
    "           \n",
    "\n",
    "        cv2.imshow(\"ImageWhite\",imgWhite)\n",
    "    key = cv2.waitKey(1)    \n",
    "    if key ==ord(\"s\"):\n",
    "        counter += 1\n",
    "        cv2.imwrite(f'{folder}/Image_{time.time()}.jpg',imgWhite)\n",
    "        print(counter)\n",
    "    elif key ==ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98af8f1e",
   "metadata": {},
   "source": [
    "# Preprocessing image dataset before feeding it  to CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c3d7b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1809 images belonging to 26 classes.\n",
      "Found 440 images belonging to 26 classes.\n",
      "{'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7, 'I': 8, 'J': 9, 'K': 10, 'L': 11, 'M': 12, 'N': 13, 'O': 14, 'P': 15, 'Q': 16, 'R': 17, 'S': 18, 'T': 19, 'U': 20, 'V': 21, 'W': 22, 'X': 23, 'Y': 24, 'Z': 25}\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define constants\n",
    "image_size = (300, 300)\n",
    "batch_size = 32\n",
    "num_classes = 26\n",
    "\n",
    "# Set the path to your dataset folder\n",
    "dataset_path = 'C:/Users/Raj/Desktop/CVIP project/ASl_data'\n",
    "\n",
    "# Create an ImageDataGenerator and specify the preprocessing steps\n",
    "data_generator = ImageDataGenerator(\n",
    "    rescale=1./255,  # Normalize pixel values to [0, 1]\n",
    "    validation_split=0.2  # Split the data into training and validation sets\n",
    ")\n",
    "\n",
    "# Load and preprocess the training set\n",
    "train_generator = data_generator.flow_from_directory(\n",
    "    dataset_path,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training'  # Specify 'training' to get the training set\n",
    ")\n",
    "\n",
    "# Load and preprocess the validation set\n",
    "validation_generator = data_generator.flow_from_directory(\n",
    "    dataset_path,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation' \n",
    ")\n",
    "\n",
    "# Verify the class indices\n",
    "print(train_generator.class_indices)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c380e2c2",
   "metadata": {},
   "source": [
    "# CNN Model to train the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5729d8e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1803 images belonging to 26 classes.\n",
      "Found 438 images belonging to 26 classes.\n",
      "Epoch 1/10\n",
      "1803/1803 [==============================] - 19s 10ms/step - loss: 0.9135 - accuracy: 0.7898 - val_loss: 1.1327 - val_accuracy: 0.8265\n",
      "Epoch 2/10\n",
      "1803/1803 [==============================] - 18s 10ms/step - loss: 0.0950 - accuracy: 0.9828 - val_loss: 1.0469 - val_accuracy: 0.8744\n",
      "Epoch 3/10\n",
      "1803/1803 [==============================] - 18s 10ms/step - loss: 0.0029 - accuracy: 0.9983 - val_loss: 1.3329 - val_accuracy: 0.8790\n",
      "Epoch 4/10\n",
      "1803/1803 [==============================] - 18s 10ms/step - loss: 0.6865 - accuracy: 0.9667 - val_loss: 1.5072 - val_accuracy: 0.8562\n",
      "Epoch 5/10\n",
      "1803/1803 [==============================] - 18s 10ms/step - loss: 0.0491 - accuracy: 0.9906 - val_loss: 1.9392 - val_accuracy: 0.8813\n",
      "Epoch 6/10\n",
      "1803/1803 [==============================] - 18s 10ms/step - loss: 0.1386 - accuracy: 0.9906 - val_loss: 10.3576 - val_accuracy: 0.8128\n",
      "Epoch 7/10\n",
      "1803/1803 [==============================] - 18s 10ms/step - loss: 0.1616 - accuracy: 0.9917 - val_loss: 8.7377 - val_accuracy: 0.8219\n",
      "Epoch 8/10\n",
      "1803/1803 [==============================] - 18s 10ms/step - loss: 0.3957 - accuracy: 0.9917 - val_loss: 5.5127 - val_accuracy: 0.8174\n",
      "Epoch 9/10\n",
      "1803/1803 [==============================] - 18s 10ms/step - loss: 0.0602 - accuracy: 0.9956 - val_loss: 3.8280 - val_accuracy: 0.8242\n",
      "Epoch 10/10\n",
      "1803/1803 [==============================] - 18s 10ms/step - loss: 2.8470e-05 - accuracy: 1.0000 - val_loss: 3.9610 - val_accuracy: 0.8242\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "image_size = (300, 300)\n",
    "batch_size = 1\n",
    "num_classes = 26\n",
    "\n",
    "dataset_path = 'C:/Users/Raj/Desktop/CVIP project/ASl_data'\n",
    "\n",
    "data_generator = ImageDataGenerator(\n",
    "    rescale=1./255,  \n",
    "    validation_split=0.2 \n",
    ")\n",
    "\n",
    "train_generator = data_generator.flow_from_directory(\n",
    "    dataset_path,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training'  \n",
    ")\n",
    "\n",
    "validation_generator = data_generator.flow_from_directory(\n",
    "    dataset_path,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'  \n",
    ")\n",
    "\n",
    "#Define the CNN model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(300, 300, 3)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_generator,\n",
    "          steps_per_epoch=train_generator.samples // batch_size,\n",
    "          epochs=10,\n",
    "          validation_data=validation_generator,\n",
    "          validation_steps=validation_generator.samples // batch_size)\n",
    "\n",
    "model.save('alphabet_classifier.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51cf9d2",
   "metadata": {},
   "source": [
    "# Testing Script\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b323d915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "1/1 [==============================] - 0s 457ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import math\n",
    "from torchvision.transforms import transforms\n",
    "from cvzone.HandTrackingModule import HandDetector\n",
    "from cvzone.ClassificationModule import Classifier\n",
    "import numpy as np\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "detector = HandDetector(maxHands =1)\n",
    "\n",
    "labels =[\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\"]\n",
    "\n",
    "classifier = Classifier(\"./model/alphabet_classifier.h5\",\"./model/labels.txt\")\n",
    "\n",
    "imgSize = 300\n",
    "offset = 50\n",
    "\n",
    "\n",
    "while True:\n",
    "    ret, img = cap.read()\n",
    "    imgout = img.copy()\n",
    "    hands, frame = detector.findHands(img)\n",
    "    if hands:\n",
    "        for hand in hands:\n",
    "            x,y,w,h = hand['bbox']\n",
    "            imgWhite = np.ones((imgSize,imgSize,3),np.uint8)*255\n",
    "            imgCrop = img[max(0,y-offset):y+h+offset,max(0,x-offset):x+w+offset]\n",
    "            aspectRatio = h/w\n",
    "\n",
    "            if aspectRatio > 1:\n",
    "                k = imgSize/h\n",
    "                wCal = math.ceil(k*w)\n",
    "                imgResize =cv2.resize(imgCrop,(wCal,imgSize))\n",
    "                imgResizeShape = imgResize.shape\n",
    "                wGap = math.ceil((imgSize - wCal)/2)\n",
    "                imgWhite[:,wGap:wCal+wGap] = imgResize\n",
    "                prediction,index = classifier.getPrediction(imgWhite)\n",
    "               # print(prediction,index)\n",
    "            else:\n",
    "                k = imgSize/w\n",
    "                hCal = math.ceil(k*h)\n",
    "                imgResize =cv2.resize(imgCrop,(imgSize,hCal))\n",
    "                hGap = math.ceil((imgSize - hCal)/2)\n",
    "                b = hGap + hCal\n",
    "                imgWhite[hGap:hCal+hGap,:] = imgResize\n",
    "                prediction,index = classifier.getPrediction(imgWhite)\n",
    "            \n",
    "            cv2.putText(imgout,labels[index],(x,y-20),cv2.FONT_HERSHEY_COMPLEX,2,(255,0,255),2)\n",
    "            cv2.imshow(\"im\",imgCrop)\n",
    "    cv2.imshow('Webcam', imgout)\n",
    "\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2eb220",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
